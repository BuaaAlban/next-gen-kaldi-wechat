## 1.背景
语言模型（LM）是语音识别框架中的重要组成部分，常常被用来减少词错率或做场景迁移（Domain adaptation）。在传统HMM-GMM的语音识别模型中，LM可以和声学模型AM进行融合，提升语音识别的准确率。近年来，端到端的语音识别模型逐渐成为主流，越来越多适用于端到端模型的语言模型融合的方式也被提出。今天，小编就带大家看看icefall中实现的一些语言模型的使用技巧。本文介绍的主要是RNNT模型上基于shallow fusion的一些解码策略，如果有小伙伴们对别的模型或者别的语言模型融合策略有兴趣，欢迎在本文留言，我们也会推出相应的教程哦~

## 2.常见的语言模型

首先，小编带大家回顾一下语言模型的基本概念。给定任意一个能被拆分为单词（字）序列的句子$\textit{w}=w_1,w_2,...,w_n$，语言模型LM的任务就是给这个序列定义一个概率，即：
$$
LM\left( \textit{w} \right)=p\left(w_1,w_2,...,w_n\right)
$$
常见的语言模型分主要分为两类：n-gram语言模型和神经网络语言模型。n-gram语言模型是早期最为常见的语言模型，它假设第k个单词出现的概率仅于之前n-1个单词相关。基于这个假设，将句子的概率根据链式法则以条件概率的形式拆分，我们可以得到n-gram语言模型的对于单词序列$\textit{w}$的概率定义：
$$
\begin{align*}
p\left(w\right)&= p\left(w_1,w_2,...,w_n\right) \\
&= p\left(w_2,...,w_n|w_1\right)p\left(w_1\right)\\

&=p\left(w_3,...,w_n|w_1,w_2\right)p\left(w_2|w_1\right)p\left(w_1\right)\\
&=\prod^{n}_{i=1}p\left(w_i|w_1,...,w_{i-1}\right)\\
&\approx \prod^{n}_{i=1}p\left(w_i|w_{i-n+1},...,w_{i-1}\right)\\
\end{align*}
$$
因为这样的语言模型对每n个单词组成的一个词组的概率进行建模，故得名n-gram语言模型。n-gram语言模型结构简单，而且可以用FST的格式表达，所以非常适合在基于FST的解码算法中使用。但是因为其在建模时的假设，使得n-gram模型不会考虑更久的历史信息，这在一定程度上限制了n-gram模型的建模能力。当然，在训练n-gram模型的时候，我们也可以使用很大的n来增加语言模型的建模能力，但语言模型的大小也会指数级增长，不方便使用。例如，当n=6的时候，如果使用大小为1000的建模单元，则需要在语言模型中存储$1000^6$种状态。

随着神经网络的发展，recurrent的模型结构在长序列上的建模能力被大家所认识，基于RNN类（RNN或LSTM）的神经网络语言模型开始出现。由于RNN不对历史信息的长度做限制，即$p\left(w_i|w_1,...,w_{i-1}\right)$对于任意i都存在定义。因此，RNN语言模型（RNNLM）能够对任意长度的单词序列进行建模，从而实现比n-gram语言模型更强的建模能力。但天下没有免费的午餐，RNNLM不支持FST模式的解码，且由于其需要对整个序列进行前向运算，速度偏慢。近年来，Transformer结构在序列建模的任务上效果卓群，大有取代RNN的趋势。Transformer LM和RNNLM一样，不对历史信息的长度做限制，而且比RNNLM训练更快，建模能力更强，进行前向计算的时候也更快，是目前最为强大的语言模型。

## 3.常见的端到端语言模型融合方式
提到端到端的语言模型融合方式，大家最先想到的肯定是shallow fusion。该方法简单有效，而且能大幅降低词错率，是很多之后涌现的语言模型融合办法的基础。本章，我们来看看shallow fusion的实现原理，以及一些基于shallow fusion的其他语言模型融合办法。

给定一个在语料空间$\mathcal{S}$上训练的RNNT模型，我们现在一个语料空间$\mathcal{T}$（$\mathcal{S}$可以和$\mathcal{T}$相同）上进行解码。假设我们有一个在$\mathcal{T}$中使用文本训练得到的语言模型，shallow fusion在解码时将语言模型的分数$p_{\text{LM}}^{\mathcal{T}}$与RNNT模型的输出分数$p_{rnnt}\left(y_u|\mathit{x},y_{1:u-1}\right)$在log空间进行线性结合：
$$
\text{score}\left(y_u|\mathit{x},y_{1:u-1}\right) = \log p_{rnnt}\left(y_u|\mathit{x},y_{1:u-1}\right) + \lambda \log p_{\text{LM}}^{\mathcal{T}}\left(y_u|\mathit{x},y_{1:u-1}\right)
$$
其中$x$为encoder的输出，$\lambda$是一个可调的超参数，控制语言模型的贡献。此处的语言模型可以是一个n-gram语言模型，也可以是一个神经网络语言模型。该方式简单直接，但是升点效果明显，也是最为常用的端到端模型的语言模型融入方式之一。

由于端到端模型在训练的时候并没有对声学模型和语言模型进行区分，现在一种常见的理论认为，端到端模型在训练时，除了声学信息外，也学到了一部分关于语言的信息。这些关于语言的信息被称为内部语言模型（internal LM， ILM），往往与训练时获得的文本强相关。由于这个内部语言模型的存在，当训练的语料空间$\mathcal{S}$和测试的语料空间$\mathcal{T}$非常不同的时候，外部语言模型的融合的效果往往会差一点。这是由于内部语言模型仅仅学习了$\mathcal{S}$内的语言信息，在进行shallow fusion时会和外部语言模型互斥，影响测试的准确率。

为了解决$\mathcal{S}$和$\mathcal{T}$不同（即cross-domain）而导致的内部语言信息和外部语言信息互斥的情况，density ratio这类办法额外在$\mathcal{S}$上训练一个语言模型，并将该语言模型的分数$p_{\text{LM}}^{\mathcal{S}}$在shallow fusion中减去，即：
$$
\text{score}\left(y_u|\mathit{x},y_{1:u-1}\right) = \log p_{rnnt}\left(y_u|\mathit{x},y_{1:u-1}\right) + \lambda_1 \log p_{\text{LM}}^{\mathcal{T}}\left(y_u|\mathit{x},y_{1:u-1}\right) - \lambda_2 \log p_{\text{LM}}^{\mathcal{S}}\left(y_u|\mathit{x},y_{1:u-1}\right)
$$
其中$\lambda_1,\lambda_2$分别控制了外部语言模型和内部语言模型的贡献。这样，density Ratior从模型外部出发，在计算最终分数的时候消除了隐含在端到端模型内的训练集上的语言信息。其中，内部语言模型往往使用一个神经网络语言模型机型模拟。上述的式子其实是通过贝叶斯公式推导出来的，有兴趣的同学可以点[这里](https://arxiv.org/pdf/2002.11268.pdf)阅读原paper。在cross-domain的场景中，density ratio的办法能够稳定的提升模型表现，且效果比单纯使用shallow fusion更好。

这时可能有小伙伴要问了，如果我的训练空间$\mathcal{S}$和测试空间$\mathcal{T}$相同（即in-domain），density ratio的办法还管用吗？答案是：有用。这是因为在训练时，端到端模型见过的文本有限，容易在训练的文本集上过拟合，因此ILM在做shallow fusion的时候反而起到了副作用。这时，如果能减轻或移除这个过拟合的ILM的分数，那么在同领域内测试时shallow fusion的效果可能会更好。Low-order Density Ratio（LODR）这一方法聚焦在RNNT模型上，认为RNNT模型的ILM仅学到了比较浅层的语言模型知识。因此，LODR使用一个低阶n-gram语言模型（比如bi-gram）模拟RNNT模型的ILM，并减去该语言模型的分数：
$$
\text{score}\left(y_u|\mathit{x},y\right) = \log p_{rnnt}\left(y_u|\mathit{x},y_{1:u-1}\right) + \lambda_1 \log p_{\text{LM}}^{\mathcal{T}}\left(y_u|\mathit{x},y_{1:u-1}\right) - \lambda_2 \log p_{\text{bi-gram}}^{\mathcal{S}}\left(y_u|\mathit{x},y_{1:u-1}\right)
$$
上式与Density ratio的式子唯一的差异就在于使用不同的办法预估$\mathcal{S}$上的语言模型信息，有兴趣的同学可以移步[这里](https://arxiv.org/pdf/2203.16776.pdf)查看原paper。该方法在in-domain和cross-domain中的表现都稳定胜过shallow fusion，而且和它的前辈density ratio不分上下。

除了刚刚介绍的这两种基于density ratio的语言模型融合办法，还有一种叫做ILME（Internal Language Model Estimation）的办法。该方法的思路很简单：既然都说端到端模型学到了一个内部语言模型，那就从端到端模型里把这个内部语言模型“找”出来不就行了！听上去是不是特别有道理~原论文中同时对RNNT模型和AED模型进行了分析，小编在这里只介绍RNNT模型的部分，有兴趣的同学可以移步[这里](https://arxiv.org/abs/2011.01991)查看更多细节。在RNNT模型中，decoder接收文本信息，再和encoder输出的语言信息进行计算，得到输出的概率。从这个角度看，RNNT模型decoder的作用和一个语言模型非常的相似。那如何获得decoder的分数呢？作者给出的方法很简单粗暴，直接将encoder的输出置零，只让decoder的输出通过joint network，便可以得到ILM的分数。当然，这么做是有一定前提条件的，作者在论文里面进行了证明，有兴趣的小伙伴可以去看看。使用ILME进行语言模型融合时，新的分数如下：
$$
\text{score}\left(y_u|\mathit{x},y\right) = \log p_{rnnt}\left(y_u|\mathit{x},y_{1:u-1}\right) + \lambda_1 \log p_{\text{LM}}^{\mathcal{T}}\left(y_u|\mathit{x},y_{1:u-1}\right) - \lambda_2 \log p_{\text{ILM}}\left(y_u|\mathit{x},y_{1:u-1}\right)
$$
式子依旧和之前的公式相差无几，只有最后一项变成了ILM的分数。ILME在in-domian和cross-domain的实验中都取得了不错的效果，相较于shallow fusion都有明显提升。

## 4.结果对比

看完了这么多语言模型融合的办法，接下来小编就给大家展示一下在icefall中的效果是怎样的。在icefall中，我们在`modified_beam_search`中实现了n-gram和RNNLM的shallow fusion，LODR和ILME三种语言模型融合策略。目前shallow fusion的策略已经合并至`master`分支（详见[这里](https://github.com/k2-fsa/icefall/pull/645)）。剩余的几种策略我们也会尽快提交合并，欢迎大家尝试~

使用icefall中`lstm_transducer_stateless2`在LibriSpeech数据集训练的**流式**模型进行in-domain测试，外部语言模型为RNNLM。在LODR实验中，我们同样使用一个bi-gram模型作为内部语言模型的预估。得到如下结果：
| decoding method |$\lambda_1$ |$\lambda_2$ |  test-clean | WERR  |  test-other | WERR |
| :--------- | :--- | --- | --- | ---|----|----: |
|  `modified_beam_search`  | - | -  |   2.73 | - | 7.15 | - |
|   shallow fusion  | 0.3 | -  |  2.42  | 11.4%  |  6.46 |  9.7% |
|  ILME  | 0.3 | 0.05  |  2.36  | 13.6% |  6.23 |  12.9% |
|   LODR  | 0.3  | 0.16  |  **2.28**  | **16.5%** |  **5.94** | **16.9%** |

使用icefall中`pruned_transducer_stateless3 + gigaspeech`在LibriSpeech数据集训练的**非流式**模型进行in-domain测试，该模型使用了Gigaspeech作为额外的训练数据。外部语言模型为RNNLM。RNNLM训练在icefall中也有实现，详见[这里](https://github.com/k2-fsa/icefall/pull/439)。在LODR实验中，我们使用一个bi-gram模型作为内部语言模型的预估。得到如下结果：

| decoding method |$\lambda_1$ |$\lambda_2$ |  test-clean | WERR  |  test-other | WERR |
| :--------- | :--- | --- | --- | ---|----|----: |
|  `modified_beam_search`  | - | -  |   2.0 | - | 4.63 | - |
|   shallow fusion  | 0.3 | -  |  2.42  | 2%  |  4.18 |  9.7% |
|   ILME  | 0.3 | 0.05  |  **1.82**  | **9.0%** |  4.10 |  11.4% |
|   LODR  | 0.4 | 0.14  |  1.83  | 8.5% |  **4.03** | **13.0%** |

可以看出，在in-domain的场景下，shallow fusion，ILME和LODR三种办法都能稳定提升模型的准确率。其中LODR的效果最为明显，ILME其次。而且在使用中我们还发现，LODR的表现更为稳定，对两个参数$\lambda_1$和$\lambda_2$的调参精度要求比ILME低。

## 5.总结

今天小编带大家回顾了一下多种在端到端模型中的语言模型融合办法，并使用icefall中的模型进行实验。结果显示，这些办法都能够显著的提高模型的准确率。大家有兴趣的话，就快去icefall试试吧~

























